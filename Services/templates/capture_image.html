{% extends "base.html" %}
{% load static %}

{% block title %}Capture Images{% endblock title %}
{% block captureactive %}active{% endblock captureactive %}

{% block body %}
{% block extra_css %}
<style>
    .bg-image {
        filter: blur(8px);
        -webkit-filter: blur(8px);
    }
</style>
{% endblock extra_css %}

<p id="captureCountDisplay">Captured Images: 0/40</p>
<video id="video" autoplay style="display: none;"></video>
<canvas id="canvas" width="600px" height="400px"></canvas>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
<script>
    let video = document.getElementById('video');
    let canvas = document.getElementById('canvas');
    let ctx = canvas.getContext('2d');
    let model;
    let captureCount = 0;
    const totalCaptures = 40;

    const setupCamera = () => {
        return navigator.mediaDevices
            .getUserMedia({ video: { width: 600, height: 400 }, audio: false })
            .then((stream) => {
                video.srcObject = stream;
                return new Promise((resolve) => {
                    video.onloadedmetadata = () => resolve(video);
                });
            });
    };

    const loadModel = async () => {
        model = await blazeface.load();
        console.log("BlazeFace model loaded.");
    };

    const sendImageToServer = (imageData, imageIndex) => {
        fetch("{% url 'capture_image' %}", {
            method: 'POST',
            headers: {
                'Content-Type': 'application/x-www-form-urlencoded',
                'X-CSRFToken': '{{ csrf_token }}'
            },
            body: `image_data=${encodeURIComponent(imageData)}&image_index=${imageIndex}`
        })
        .then(response => response.text())
        .then(data => {
            console.log(data);
        })
        .catch(error => console.error('Error:', error));
    };

    const captureLoop = async () => {
        if (captureCount >= totalCaptures) return;

        const predictions = await model.estimateFaces(video, false);

        if (predictions.length > 0) {
            const face = predictions[0];
            const [x1, y1] = face.topLeft;
            const [x2, y2] = face.bottomRight;

            const width = x2 - x1;
            const height = y2 - y1;

            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            // Create a temporary canvas for the cropped face
            const faceCanvas = document.createElement('canvas');
            faceCanvas.width = width;
            faceCanvas.height = height;

            const faceCtx = faceCanvas.getContext('2d');
            faceCtx.drawImage(canvas, x1, y1, width, height, 0, 0, width, height);

            const imageData = canvas.toDataURL('image/jpeg');

            sendImageToServer(imageData,captureCount);

            captureCount++;
            document.getElementById('captureCountDisplay').innerText = `Captured Images: ${captureCount}/${totalCaptures}`;
        }

        // Delay before next capture
        setTimeout(captureLoop, 300); // 300ms delay
    };

    const start = async () => {
        await setupCamera();
        await loadModel();
        captureLoop();
    };

    start();
</script>

{% endblock body %}
